494
ИНТЕЛЛЕКТУАЛЬНЫЙ АНАЛИЗ ПОТОКОВЫХ ДАННЫХ В БЕСПРОВОДНЫХ СЕНСОРНЫХ СЕТЯХ С ПОМОЩЬЮ ДЕРЕВЬЕВ РЕШЕНИЙ 
2 Передовые методы потокового машинного обучения для деревьев решений 
2.3 Задача классификации потоковых данных с помощью дерева Хеффдинга 
----------
Классические алгоритмы машинного обучения для деревьев решения могут одновременно сохранить все обучающие данные основную память. Если данные хранятся в постоянной памяти ЭВМ, то алгоритмы машинного обучения  могут неоднократно считывать тренировочнык данные с диска последовательно, что непомерно дорого при обучении сложных деревьев.
Цель дерева Хеффдинга: разработка такого обучающего дерева решений, в котором для обучения каждый пример читается только однажды, и используются малые постоянные времени для обработки этих примеров.
Для того, чтобы найти лучший атрибут в узле, возможно будет достаточно рассматривать только небольше подмножество обучающих примеров, которые проходят через этот узел. В данном потоке примеров можно использовать первый в качестве корневого атрибута.
После того, как выбран корневой атрибут, последовательные примеры передаются в соответствующие листья, и используются для выбора следующих атрибутов, и так далее рекурсивно.
Границы Хеффдинга помогают решить вопрос: сколько примеров достаточно для обучения в каждом узле. Или иначе, сколько экземпляров необходимо для достижения определенного уровня доверия.
Дано:
* r -> вещественная случайная величина, с диапазоном R ;
* n -> число независимых наблюдений ;
*  -> среднее значение, вычисленное по формуле из n независимых наблюдений.
Связанная с Хеффдингом теория утверждает, что с вероятностью 1-d, истинное среднее значение переменной не меньше . А также  определяется по следующей формуле:
,                                                   (1)
Следует заметить способность алгоритма давать одни и те же результаты независимо от распределения вероятностей, порождающего наблюдения. Однако число наблюдений, необходимых для достижения определенных значений  d  а также Î  различаются по вероятностным распределениям.
Вот конкретный пример, который поможет нам понять концепцию: предположим, что разница в информации (индекс Gain) между двумя атрибутами A и B (коэффициент усиления информации A больше, чем B) равен 0,3 и . Это означает, что в будущем минимальная разница между коэффициентом усиления A и B будет, по меньшей мере, .
Другими словами, один признак превосходит другие c вероятностью 1-d , при условии, что наблюдаемая разница в получении информации больше,
чем e.
Входные параметры алгоритма:
* S -> последовательность примеров;
* X -> множетво дискретных атрибутов;
* G(.) -> функция расщепления;
* δ -> единица минус желаемая вероятность выбора правильного атрибута в любом узле.
Выходные параметры алгортма:
HT (Hoeffding Tree) -> дерево решений.
Пусть G(Xi) - эвристическаямера для выбора тестовых атрибутов (т.е. индекс расщепления - Information Gain, Gini Index). Xa : атрибут с наиболее высоким значенем значением индекса после перебора n примеров. Xb : атрибут со вторым по счету индексом ращепления после перебора n примеров.
Дано желаемое значение d, if  после перебора n примеров в узле,
Границы Хеффдинга гарантируют правдоподобный результат                                 с вероятностью 1-d , при условии, что
Этот узел может быть расщеплен с помощью атрибута  Xa, и последующие примеры будут направлены в новые листья.
Алгоритм выполняется в следующей последовательности:
* Вычисление разницы в информации (индекс Gain) для атрибутов и определяем лучшие два атрибута;
* Считаем пустым (“null”) атрибут, не приводящий к расщеплению узла (Pre-pruning – предварительная обрезка дерева);
* Для каждого узла проверяется условие: ;
* Если условие удовлетворяется, создаем дочрние узлы, как результаты условной вершины;
* Если не удовлетворяется – из потока извлекаются еще примеры и выполняются вычисления пока условие не будет выполнено.