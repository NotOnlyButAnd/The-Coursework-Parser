414
ПРИМЕНЕНИЕ МЕТОДОВ ИНТЕЛЛЕКТУАЛЬНОГО АНАЛИЗА ДАННЫХ ДЛЯ ЗАДАЧ МЕДИЦИНСКОЙ ДИАГНОСТИКИ 
1 Основные понятия задачи классификации 
1.5. Наивный Байесовский классификатор 
----------
Байесовский подход к классификации основан на теореме, утверждающей, что если плотности распределения каждого из классов известны, то искомый алгоритм можно выписать в явном аналитическом виде. Более того, этот алгоритм оптимален, то есть обладает минимальной вероятностью ошибок.
Байесовские классификаторы основываются на формуле Байеса:
,   			   	      (1)
где P(y|x) – апостериорная вероятность данного класса c (т.е. данного значения целевой переменной) при данном значении признака x, P(y) – априорная вероятность данного класса, P(x|y) – правдоподобие, т.е. вероятность данного значения признака при данном классе, P(x) – априорная вероятность данного значения признака. Так как решается задача классификации, а не непосредственное вычисление вероятности, то можно пренебречь вычислением значения P(x). Это объясняется тем, что есть необходимость только в нахождении такого , что . Из формулы (1) видно, что значение P(x) не влияет на значение y, а это значит, что вычислять его нет необходимости.
На практике плотности распределения классов, как правило, не известны. Их приходится оценивать (восстанавливать) по обучающей выборке. В результате байесовский алгоритм перестаёт быть оптимальным, так как восстановить плотность по выборке можно только с некоторой погрешностью. Чем короче выборка, тем выше шансы подогнать распределение под конкретные данные и столкнуться с эффектом переобучения.[3]
Байесовский подход к классификации является одним из старейших, но до сих пор сохраняет прочные позиции в теории распознавания. Он лежит в основе многих достаточно удачных алгоритмов. Примером такого алгоритма может послужить EM-алгоритм, который служит для нахождения оценок максимального правдоподобия и для разделения смесей распределений.
Наивный байесовский классификатор — специальный частный случай байесовского классификатора, основанный на дополнительном предположении, что объекты  описываются n статистически независимыми признаками. Основные преимущества наивного байесовского классификатора — простота реализации и низкие вычислительные затраты при обучении и классификации. В тех редких случаях, когда признаки действительно независимы (или почти независимы), наивный байесовский классификатор (почти) оптимален.[5]
Достоинства байесовского классификатора:
* хорошо работает на практике, когда данные имеют вероятностную природу;
* простая реализация;
* скорость работы;
* разделяет объекты достаточно простыми, но нетривиальными разделяющими поверхностями (в случае нормальных распределений) [4].
Алгоритм построения наивного байесовского классификатора.
Данный алгоритм основывается на предположении, что все признаки независимы и распределены в соответствии с нормальным законом распределения.
Этап обучения модели состоит в вычислении матожидания и среднеквадратичного отклонения каждого признака в каждом классе.
Этап получения предсказания для объекта x состоит из следующих шагов:
1)  Для всех классов  выполняются шаги 2-4
2)  Вычисляется априорная вероятность появления класса по классическому определению вероятности.
3)  Вычисляется суммарной апостериорная вероятность признаков x при условии класса y, т.е. вероятность получить x при нормальном распределении с матожиданием и среднеквадратичным отклонением, вычисленными на этапе обучения модели.
4)  Возвращается класс с максимальным значением апостериорной вероятности.