266
ИЗУЧЕНИЕ МЕТОДОВ КЛАССИФИКАЦИИ ДОКУМЕНТОВ ДЛЯ ОПРЕДЕЛЕНИЯ ЭМОЦИОНАЛЬНОЙ ОКРАСКИ ТЕКСТА
1 Подходы к сентиментному анализу 
1.3 Модели для построения векторного представления 
1.3.2 Word2Vec 
----------
В данной модели для получения векторов используется машинное обучение. Изначально задается размерность векторов, которые заполняются случайными величинами, во время обучения значения компонент векторов буду меняться, причем вектор каждого слова будет максимально схож с векторами типичных соседей и максимально отличаться от векторов слов, которые соседями данному слову не являются. Компоненты векторов никак не связаны с конкретными словами из словаря.
Алгоритм построения модели [4]:
1)  составляется словарь терминов, встретившихся во всех документах. Его размер практически не ограничивается, исключаются только слова, имеющие наименьшую встречаемость;
2)  каждому термину в словаре сопоставляется частота встречаемости во всех документах;
3)  для кодирования словаря строится дерево Хаффмана;
4)  производится субдискретизация частых слов;
5)  для этих слов применяется один из алгоритмов: CBOW (Continuous Bag-of-Words) или Skip-gram;
6)  применяется нейронная сеть прямого распространения с функцией активации иерархический softmax или негативное семплирование (negative sampling).
На рисунке 1 представлена архитектура CBOW.
Рисунок 1 – Архитектура CBOW
Она аналогична нейронной сети прямого распространения, где нелинейный скрытый слой удаляют, а проекция слоя является общей для всех слов, таким образом, все слова находятся в одинаковом положении. Очень похоже на модель «мешок слов» (Bag-of-Words): порядок слов никак не влияет на проекции. Задача архитектуры при обучении модели – предсказать слово по имеющемуся контексту. CBOW в отличие от BOW использует непрерывное распределенное представление контекста, поэтому и имеет такое название.
Архитектура Skip-gram, изображенная на рисунке 2, похожа на CBOW, но вместо того, чтобы предсказывать текущее слово исходя из окружения, она пытается для текущего слова предсказать контекст – слова, стоящие в пределах определенного диапазона (до и после данного слова). Увеличение контекстного окна улучшает качество получаемых векторов, но заметно увеличивает вычислительную сложность.
Рисунок 2 – Архитектура Skip-gram