1493
ИССЛЕДОВАНИЯ МЕТОДОВ ПРИНЯТИЯ РЕШЕНИЙ
2. ДЕРЕВЬЯ РЕШЕНИЙ 
2.1. Алгоритмы деревьев решений 
----------
Дерево решений – один из способов представления информации, в виде различных уровней и связей. Зачастую, строится по иерархическому принципу:
* главная цель помещается в корень дерева
* методы достижения этой цели являются ветвями деревьев (первого, второго и последующих уровней).
Основное отличие деревьев решений от других методов заключается в том, что проводимое исследование основывается на логических рассуждениях и вычислениях. Деревья решений – это один из методов построенных на основе правил вывода. Эти системы часто называют системами прямого логического вывода, так как, анализ начинается с фактов, а в результате получаем тот или иной вывод.
Логический порядок построения дерева решений состоит в следующем:
в наиболее простом виде дерево решений – это способ представления правил в иерархической, последовательной структуре. Основа данной структуры являются ответы «Да» или «Нет» на различные вопросы.
Стоит отметить, что бинарные деревья являются самым простым, частным случаем деревьев решений, так как могут иметь только 2 ветви от узла. В остальных деревьях, ответов и, соответственно, ветвей дерева, выходящих из внутреннего узла, может быть больше двух.
В методе построения решений, есть несколько алгоритмов выбора очередного атрибута.
Рассмотрим некоторые из них:
1.  Алгоритм ID3 – выбор атрибута происходит на основание увеличения информации, либо на основании индекса Gain.
2.  Алгоритм С4.5 (усовершенствованная версия ID3) – выбор атрибута базируется на основании нормализованного прироста информации.
3.  Алгоритм CART – алгоритм предназначен для работы с бинарными деревьями, и делит на каждом шаге ответы напополам: по одной ветви выполняются примеры, не удовлетворяющие условию, а по другой удовлетворяющие.
4.  Алгоритм CHAID (Хи-квадрат) – алгоритм выполняет многоуровневое разделение при расчете классификации деревья.
5.  Алгоритм Mars – расширяет деревья решений для улучшения обработки цифровых данных.
Рассмотрим их подробнее:
* Алгоритм ID3
Возьмем, например, атрибут, который принимает 3 значения: X, Y, Z.
При разбиении множества алгоритм создаст три узла T1(X), T2(Y), T3(Z), в первый из них будет помещены все записи со значение Х, во второй Y, в третий С.
Процедура повторяется рекурсивно до тех пор, пока не останутся только примеры одного класса, после чего они будут объявлены листами и ветвление прекратиться. Самым проблемным этапом является выбор атрибута, по которому производится разбиение. Алгоритм ID3 справляется с этим недугом с помощью увеличение информации или уменьшение энтропии.
Далее, рассмотрим математическую модель алгоритма.
Пусть Т множество текстов, а их количество – мощность данного множества |T|. Множество классов будет обозначаться, как С={C1, C2, … ,Ck}, а множество признаков A={A1, A2, … ,Am}.
По каждому признаку Аi можно разбить множество Т на подмножества Т1,Т2 , … , Тn.
Пусть F(Cj, T) – количество текстов из некоторого множества Т, лежащих в одном классе Cj . Тогда вероятность того, что случайным образом выбранный текст окажется из множество Т и принадлежащим классу Сj .
(1)
Тогда энтропия множества Т имеет вид:
(2)
Условная энтропия множества текстов Т при рассматриваемом признаке Х есть:
(3)
После для каждого из признаков вычисляется объем информации
I(X) = H(T) – H(T|X)                                             (4)
Существует 2 варианта разбиения.
Если Аi – номинальный признак, то количество значение признака Аi, будет соответствовать количеству подмножеств Т.
Если Аi - числовой признак, то множество Т разбивается на два подмножества. При это необходима выбрать порог разбиения, по которому будут сравниваться все значения признака.
v = {v1, v2, … , vn}
Для начала следует отсортировать значения. Тогда значение, лежащие между vi  и vi+1, делит все значения на два множества и в качестве порога можно выбрать среднее значение между vi  и vi+1.
(5)
Следует, можно сделать вывод, что имеется n-1 потенциальное пороговое значение. Так как для номинального признака имеется всего один вариант разбиения, а для числового признака количество вариантов разбиения равно количеству порогов. Если имеется u номинальных признаков и v числовых, то в каждой вершине разбиения можно расписать O способами, где
O = u + v(n-1).                                                    (6)
* Алгоритм С4.5
Представляет собой усовершенствованный вариант алгоритма ID3.  Отличие появляется в критерии разбиения множества на подмножества.
Критерий разбиения (6) имеет недостаток, он выбирает признаки, которые имеют много значений, так как при разбиении по такому признаку получаются подмножества, содержащие минимальное число текстов. Проблему можно решить при помощи некоторой нормализации.
,                                     (7)
Где
Х = {T1, T2, … , Tn} и .                                 (8)
Выражение (10) оценивает потенциальную информацию, полученную при разбиении множества Т на подмножества n.
Тогда критерием разбиения будет
(9)
* Алгоритм CART
CART (Classification And Regression Tree) – переводится как «Дерево Классификации и регрессии». Существует также несколько модифицированных версий IndCART(отличается использованием другого способа обработки пропущенных значений, имеет другие параметры отсечения) и BD-CART(вместо того чтобы использовать обучающий набор данных для определения разбиений, использует его для оценки распределения входных и выходных данных, а затем использует эту оценку для разбиения). В данном алгоритме любой узел дерева имеет двух потомков. На каждом шаге построения дерева атрибут, делит множество примеров на две части – часть, в которой выполняется правило (потомок - right) и часть, в которой правило не выполняется (потомок - left). Для выбора оптимального атрибута необходимо использовать функцию оценки качества разбиения.
Оценочная функция базируется на интуитивной идее уменьшения неопределенности в узле. (Например, рассмотрим пример с двумя классами и узлом, имеющим по 60 примеров одного класс, если найти разбиение делящие данные на две подгруппы 50:5 в одной и 10:55 в другой, то «нечистота уменьшится», но полностью она исчезнет, когда результатом будет 60:0 в одной и 0:60 в другой). В алгоритме CART эта идея реализована в индексе Gini. Если набор данных T содержит данные n классов, тогда индекс Gini определяется, таким образом:
(10)
где параметр  – вероятность класса i в T.
Существует так же индекс Gini для ситуации, когда набор T разбивается на две части T1 и Т2 с числом примеров в каждом N1 и  N2, тогда показатель будет равен:
.                                 (11)
Отметим, что наилучшим считается разбиение, для которого индекс получается минимальным.
Правило разбиения в алгоритме CART схоже с алгоритмов ID3. Если переменная числового типа, то в узле формируется правило xi <= c (где с-некоторый порог, который чаще всего формируется, как среднеарифметическое двух соседних значений в множестве). Если переменная категориального типа, то в узле формируется правило xi V(xi), где V(xi) – некоторое непустое подмножество множества значений переменной xi в обучающей выборке. Таким образом, для n значений числового атрибута алгоритм сравнивает n-1 разбиений, а для категориального (2n-1 – 1). На каждом шаге построения алгоритм сравнивает все возможные варианты и выбирает лучший атрибут и наилучшие разбиение.
Механизм отсечения дерева, оригинальное название minimal cost-complexity tree pruning, - этот механизм является значимым отличие алгоритма CART от других. Отсечение решает сразу две значительные проблемы:
* получение дерева оптимального размера;
* получение точной оценки вероятности ошибки.
Обозначим |T| - число листов дерева, R(T) – ошибка классификации дерева. Определим полную стоимость дерева Т так:
,                                       (12)
где α – некоторый параметр (возможно изменение от 0 до +∞).
Полная стоимость дерева имеет две составляющие – ошибку классификации дерева и штраф за его сложность. Можно заметить, что с увеличением параметра α будет расти полная стоимость дерева. Поэтому в зависимости от α, менее ветвистое дерево, дает большую классификацию, может стоить меньше, чем дерево более ветвистое, но имеющие меньшую ошибку.
Определим  – максимальное по величине дерево, которое предстоит обрезать. Если задать значение α, то будем иметь наименьшее поддерево Т(α), для которого выполняются несколько условий:
* Условие сообщает, о том, что не существует такого поддерева  , которое имело бы наименьшую стоимость, чем  при заданном значение α;
* Если существует больше одного поддерева, имеющих полную стоимость, то мы выбираем наименьшее дерево.
Несмотря на то, что параметр α имеет бесконечное множество значений, существует конечное множество поддеревьев дерева , поэтому можно создать последовательность поддеревьев:
T1 > T2 > T3 >...> {t1},                                          (13)
(где t1 – корневой узел дерева) такую,  что  – наименьшее поддерево для α ∈ [αk, αk+1).
Первое дерево в этой последовательности – наименьшее поддерево дерева  имеющую такую же ошибку классификации, получим, что . Другими словами, если разбиение идет до тех пор пока в каждом узле не останется только один класс, то  .
Выбор финального дерева заключается в выборе лучшего дерева из последовательности деревьев. Стоит отметить, что при отсечении ветвей, дерева использовались только первоначальные данные (если быть точнее, то даже не сами данные, а количество примеров, каждого класса). Зачастую, наиболее эффективным вариантом проверки финального дерева является тестирование на тестовой выборке (качество тестирование в таком случае зависит от объема тестовой информации). Иногда можно наблюдать некоторые ошибки в составление, чтобы уменьшить эту нестабильность, CART использует (1 - ) – правило: выбирается минимальное по размеру дерево с  в пределах интервала  [min , min  +], где - ошибка классификации дерева, а  – стандартная ошибка, являющаяся оценкой ошибки реальной:
,                                       (14)
где - число примеров.
Алгоритм CART хорошо сочетает в себе результативность построенных моделей, а, так же, высокую скорость их построения. Так же имеет свои уникальные методы обработки пропущенных значений и построения оптимального дерева.
* Алгоритм CHAID (Хи-квадарат)
Чтобы построить дерево решений с помощью данного алгоритма, нужны зависимая (имеет две категории) и независимая (используется для разделения атрибутов на группы) переменные.
CHAID основан на критерии Хи-квадрат, которые позволяет нам определять, связаны ли статистически две переменные. Алгоритм определяет, как лучше сгруппировать категории каждой независимой переменной, так чтобы значение Хи-квадрат было максимальным. Это позволяет определить насколько независимые переменные отличны друг от друга.
Таким образом, получаем дерево, листьями которого являются группы с максимально различными значениями зависимой переменной. По такому дереву легко определить в какой из группы интересующий нас признак максимален.